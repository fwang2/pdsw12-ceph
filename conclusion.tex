\section{Observations and Conclusions}
\label{sec:conclusion}

Below are our preliminary observations from mostly performance perspective:

\begin{itemize}

  \item Ceph is built on the assumption that the underlying hardware components
are unreliable, with little or no redundancy and failure detection capability.
This assumption is not valid for high-end HPC centers like ours. We have
disabled replication for pools, we haven't measured and quantified
processing overhead and we do not know yet if this would be significant.

  \item Ceph performs \textbf{metadata + data} journaling, which is fine for
host systems that has locally attached disk. However, this presents a problem
in DDN SFA10K-like hardware, where the backend LUNs are exposed as block
devices through IB over SRP protocol. The journaling write requires twice the
bandwidth compared to Lustre-like meta data-only journaling mechanism. For Ceph
to be viable in large-scale capability HPC environments like ours, journaling
operations will need to further design and engineering.

  \item We observed consistent results and linear scalability at the RADOS
level. However, we did not observe the same at the file system level. We have
experienced large performance swings during different runs, very low read
performance when transfer size is small, and I/O errors tend to happen when
system is under stress (more clients and large transfer sizes). These are not
particularly reproducible results, but it suggests that there are opportunities
for code improvement and maturation.

\end{itemize}

