\section{Testbed Environment Description}
\label{sec:testbed}

We used Data Direct Networks' (DDN) SFA10K as the storage backend for this
evaluation. SFA10K organizes disks into various RAID levels by two
active-active RAID controllers. The exported RAID groups (LUNs) by these
controllers are driven by four server hosts (oss-[1-4]).  Each server host has two InfiniBand
(IB) QDR connections to the storage backend.  We used a single dual-port
Mellanox ConnectX IB QDR card per host.  By our calculation, this setup is
adequate to drive the SFA10K at its maximum theoretical throughput (roughly 12
GB/s). Server hosts and SFA10K connection diagram is illustrated in
Figure~\ref{fig:ddn-sfa10k}.

\begin{figure}[htb]
\centering
\includegraphics[width=3in]{figs/sfa10k}
\caption{DDN SFA10K hardware and host connection diagram}
\label{fig:ddn-sfa10k}
\end{figure}


Our SFA10K system consists of 200 SAS drives and 280 SATA drives, hosted in a
10-tray configuration. Each DDN SA4601 disk tray can host up to 60 disks and
has two SAS links per controller (four in total). The SATA and SAS disks in our
testbed are distributed evenly over ten SA4601 disk trays (20 SAS and 28 SATA
disks per tray). Each SFA10K RAID controller has two RAID processors and each
RAID processor hosts a dual-port IB QDR card. The disks are organized into RAID
groups by the RAID processors (software RAID) and then exported to connected
server hosts over the IB connections. Since each DDN SFA10K controller is a
stand-alone computer system running Linux, CPU and memory affinity while
organizing disks into RAID groups and exporting them is crucial for obtaining
top performance out of an SFA10K system.  

Our Ceph testbed employs a collection of nodes, including the server hosts
(oss-[1-4]). These nodes and their roles are summarized in
Table~\ref{tbl:ceph-test-nodes}. In the following discussion, we use
``servers'', ``osd servers'', ``server hosts'' interchangeably. We will
emphasize with ``client'' prefix when we want to distinguish it from above.

\begin{table}[!t]
\centering
\caption{Support nodes involved in Ceph testbed}
\label{tbl:ceph-test-nodes}
    \begin{tabular}{ll}
    \hline
    Node & Role \\
    \hline
    tick-mds1 & Ceph monitor node \\
    spoon46 & Ceph MDS node \\
    tick-oss[1-4] & Ceph OSD servers \\
    spoon28-31, spoon37-41 & Ceph client nodes \\
    \hline
    \end{tabular}
\end{table}

All hosts  (client and servers) were configured with Redhat 6.3 and kernel
version 3.5.1 initially, and later upgraded to 3.9 (rhl-ceph image), Glibc
2.12 with syncfs support, locally patched.  We used the Ceph 0.48 and 0.55
release in the initial tests, upgraded to 0.64 and then to 0.67RC for a final
round of tests.

%For a complete a list of hosts that are running ceph images, one can execute:

%\begin{Verbatim}
%$ grep "rhel6-ceph" /etc/gedi/MAC.info
%\end{Verbatim}

