\section{Background}
\label{sec:background}

Ceph\cite{Weil:2006:Ceph} is a distributed storage system designed for
scalability, reliability, and performance.  The system is based on a
distributed object storage service called RADOS (reliable autonomic distributed
object store) that manages the distribution, replication, and migration of
objects.  On top of that reliable storage abstraction Ceph builds a range of
services, including a block storage abstraction (RBD, or rados block device)
and a cache-coherent distributed file system (CephFS).

Data objects are distributed across Object Storage Devices (OSD), which refers
to either physical or logical storage unit, using CRUSH\cite{Weil:2006:Crush},
a deterministic hashing function that allows administrators to define flexible
placement policies over a hierarchical cluster structure (e.g., disks, hosts,
racks, rows, datacenters).  The location of objects can be calculated based on
the object identifier and cluster layout (similar to consistent
hashing~\cite{karger1997consistent}), thus there is no need for a metadata
index or server for the RADOS object store.
%Further, because CRUSH provides an authoritative view of data placement,
%storage daemons can coordinate directly to handle data replications,
%recovery, or object migration in the face of failure or cluster topology
%changes.  
A small cluster of monitors (ceph-mon daemons) use Paxos to provide consensus
on the current cluster layout, but do not need to explicitly coordinate
migration or recovery activities.

CephFS builds a distributed cache-coherent file system on top of the object
storage service provided by RADOS.  Files are striped across replicated
storage objects, while a separate cluster of metadata servers (ceph-mds
daemons) manage the file system namespace and coordinate client access to
files.  

Ceph metadata servers store all metadata in RADOS objects, which provides a
shared, highly-available, and reliable storage backend.  Unlike many other
distributed file system architectures, Ceph also embeds inodes inside
directories in the common case, allowing entire directories to read from RADOS
into the metadata server cache or prefetched into the client cache using a
single request.

Client hosts that mount the file system communicate with metadata
servers to traverse the namespace and perform file I/O by reading and writing
directly to RADOS objects that contain the file data.  The metadata server
cluster periodically adjusts
the distribution of the namespace across the MDS cluster by migrating
responsibility for arbitrary subtrees of the hierarchy between a dynamic pool
of active ceph-mds daemons.  This dynamic subtree
partitioning~\cite{Weil:2004:dynamic} strategy is both adaptive and highly
scalable, allowing additional metadata server daemons to be added or removed
at any time, making it ideally suited both for large-scale workloads with
bursty workloads or general purpose clusters whose workloads grow or contract
over time.

\begin{comment}
In comparison to other parallel file
systems, Ceph has a number of distinctive features:

\begin{itemize}
 
\item Ceph has an intelligent and powerful data placement mechanism, known as
  CRUSH. The CRUSH algorithm allows a client to pre-calculate object
  placement and layout while taking into consideration of failure domains and
  hierarchical storage tiers.
  
  \item From the start, Ceph's design anticipated managing meta data and the
  name space with a cluster of meta data servers. It utilized a dynamic subtree
  partitioning strategy to continuously adapt meta data distribution to current
  demands.

  \item Ceph's design assumes that the system is composed of unreliable
  components; fault-detection and fault-tolerance (e.g., replication) are the
  norm rather than the exception. This is in line with the expectations and
  future directions of Exascale computing.

  \item Ceph is built on top of a unified object management layer, RADOS. Both
  meta data and the file data can take advantage of this uniformity. On top of
  RADOS, Ceph build and project a host of other features such as RESTful
  interface, S3 and Swift-compliant API, cloud integration.

  \item Most of the Ceph processes reside in user-space. Generally speaking,
this makes the system easier to debug and maintain. The client-side support has
long been integrated into Linux mainline kernel, which eases the deployment and
out-of-box experience.

\end{itemize}
\end{comment}

While we are interested in Ceph for its ability to support alternative
workloads that are not easily accomplished with Lustre, in this study we are
investigating the use Ceph for future large-scale scientific HPC storage
deployments. This paper presents our experience, results, and observations in
measuring the scalability and performance of Ceph.  

%Additionally, Ceph is under
%rapid development, and in between releases, we often
%experienced different stability and performance outcomes.  
%We will try to make
%clear in the writing when such changes occurred.
%As interesting and feature-rich as Ceph may be,  scalability and performance
%are our top priorities due to the unique requirements our environment.  As part
%of this study, we set up a dedicated testbed within OLCF to  evaluate Ceph.
%Our goal was to investigate the feasibility of using the Ceph for future
%large-scale scientific HPC storage deployments. This paper presents our
%experience, results, and observations.  While evaluating our results, please
%keep in mind that Ceph is still a relatively \textit{young} parallel file
%system and its code base is changing rapidly. In between releases, we often
%experienced different stability and performance outcomes.  We will try to make
%clear in the writing when such changes occurred.

%This paper is organized as follows. Section~\ref{sec:testbed} gives an
%overview on our general test and evaluation methodology as well as testbed
%environment; Following it, Section~\ref{sec:baseline} establishes the baseline
%performance and expectations for all critical components on the data path;
%Section~\ref{sec:ceph-initial} discusses our early runs, results, and issues
%bottom up: from middle tier RADOS object layer to the file system-level and
%then meta data performance. In Section~\ref{sec:ceph-tuning} we highlight the
%investigative and tuning effort, we compare results before and after, and how
%the process eventually bring the system performance to a respectable state.
%Finally, Section~\ref{sec:conclusion} summarizes our findings, observations,
%and future works.
