\section{Improving RADOS Performance}

\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{parametric}
\caption{Evaluating parameter impact through sweeping test}
\label{fig:parametric}
\end{figure}


After the initial test results, we tried various combinations of tweaks
including changing the number of filestore op threads, putting all of the
journals on the same disks as the data, doubling the OSD count, and upgrading
Ceph to a development version which reduces the seek overhead caused by
\texttt{pginfo} and \texttt{pglog} updates on XFS (these enhancements are now
included as of the Ceph Cuttlefish release, v0.61).  The two biggest
improvements resulted from disabling CRC32c checksums and increasing the OSD
count on the server.  With these changes, we are seeing better results.

We ran a script written by Inktank for internal Ceph testing to perform
sweeps over Ceph configuration parameters to examine how different
tuning options affect performance on the DDN platform. The result of this
parameter probing is illustrated in Figure~\ref{fig:parametric}. Please refer
to Appendix E for explanations of these probed parameters.


As a result of this testing, we improved performance slightly by
increasing the size of various Ceph buffers and queues, enabling AIO journals,
and increasing the number of OSD op threads.


\subsection{Disable Cache Mirroring on Controllers}

During a second round of test performed by Inktank, we noticed a dramatic drop
on RADOS performance: even though write throughput on individual server met the
expectation, it did not scale across servers.

We spent a significant amount of time
investigating this phenomenon. Ultimately, we were able to replicate this finding
when running concurrent disk throughput tests directly on the servers without
Ceph involved. The second RAID processor on each DDN controller would max out when
three or more LUNs were written concurrently. It turns out the root of the problem
was a regression on DDN firmware update -- in particular, the cache
mirroring was not behaving as it should.\footnote{DDN recently released a new
firmware version and we were told the issue has been fixed. Unfortunately, we didn't get
a chance to verify it during our test cycle.}

%% SCOTT - is running with cache mirroring off and option for a production system or not?
%% What are the consequences? Did DDN eventually provide a fix? If yes, were we able to test
%% with it or not?

%% FEIYI: add footnote to clarify the issue.

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{rados-after-ddn}
\caption{Evaluating RADOS bench after disabling cache mirroring}
\label{fig:rados-ddn-mirror-disabled}
\end{figure}


With cache mirroring disabled, write performance when using all four servers
improved dramatically, as illustrated in
Figure~\ref{fig:rados-ddn-mirror-disabled}. With BTRFS, for example, we hit over
5.5 GB/s from the clients.  When accounting for journal writes, that is over
11 GB/s to the disks and very close to what the DDN chassis is capable of doing. 
Unfortunately, read performance did not scale as well.


\subsection{Disable TCP autotuning}

During these tests, a trend that previously had been seen became more
apparent.  During reads, there were periods of high performance followed by
periods of low performance or outright stalls that could last for up to 20
seconds at a time.  After several hours of diagnostics, Inktank observed that
outstanding operations on the clients were not being shown as outstanding on
the OSDs.  This appeared to be very similar to a problem Jim Schutt at Sandia
National labs encountered with TCP autotuning in the Linux
kernel.\footnote{\url{http://marc.info/?l=ceph-devel&m=133009796706284&w=2}}
TCP auto tuning enables TCP window scaling by default and automatically
adjusts the TCP receive window for each connection based on link conditions
such as bandwidth delay product. We have observed this will make a notable
improvement on Ceph read performance, as the results shown in
Figure~\ref{fig:rados-tcp-auto-disabled}.


Luckily, the fix was fairly straight forward by issuing the following command on all nodes:

\begin{Verbatim}
     echo 0 | sudo tee /proc/sys/net/ipv4/tcp_moderate_rcvbuf
\end{Verbatim}

Recent versions of Ceph work around this issue by manually controlling the TCP
buffer size.  The testing at ORNL directly influenced and motivated the creation
of this feature!

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{rados-after-ddn-tcptune}
\caption{Evaluating RADOS bench after TCP auto tuning disabled}
\label{fig:rados-tcp-auto-disabled}
\end{figure}



%\begin{figure}[htb]
%\includegraphics[width=5in]{rados-064-oss}
%\end{figure}

\subsection{Repeating RADOS Scaling Test}

We now repeated the previous RADOS scaling tests with these improvements in place.
The first test was done on a single node with RADOS Bench to see how close the
underlying object store could get to the node hardware limitations as the number
of OSDs/LUNs used on the node increased. Note all the tests performed were against
XFS-formatted storage.

%% SCOTT - the title in the figure needs to change IO to I/O

%% SCOTT - how do writes inc. journals exceed the client network max? Dual ports
%% on the single server?

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{rados-064-osd}
\caption{RADOS Bench Scaling on \# of OSD, Ceph 0.64, 4 MB I/O, 8 Client Nodes}
\label{fig:rados-064-osd}
\end{figure}

In the single server case as shown in Figure~\ref{fig:rados-064-osd}, ``Writes
(including Journals)'' refers to how much data is actually being written out the
DDN chasis, and blue line is how much data the clients are writing.
We observe that performance gets very close to the hardware limits at roughly 9
OSDs per server and then mostly levels out.

We also repeated tests looking at RADOS Bench performance as the number of OSD
server nodes increases from one to four. The results are summarized in
Figure~\ref{fig:rados-064-oss}. As the number of nodes increases, performance
scales nearly linearly for both reads and writes.

%% SCOTT - why does the client network max scale in figure 12 but not figure 11?
%% Do they no measure the same thing (i.e. a single server)? If not, the text
%% needs to make it more clear.

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{rados-064-oss}
\caption{RADOS Bench Scaling on number of servers, Ceph 0.64, 4 MB I/O, 8 client
nodes}
\label{fig:rados-064-oss}
\end{figure}



\section{Improving Ceph File System Performance}
\label{sec:improve-ior}

The initial stability issues mentioned in Section~\ref{sec:ior-initial} are
fixed by migrating from Ceph version 0.48/0.55 to 0.64, the latest stable version at the
time of writing this report.  Upgrading to the latest stable Ceph release
allowed us to run a full IOR parameter sweep for the first time since we
started evaluating the performance and scalability of the Ceph file system.
This is another sign of how much Ceph development is currently in flux.

Another fix introduced by Ceph version 0.64 was in pool creation.  The default
data pool used by previous Ceph version were set to 2x replication by mistake.
This potentially halved the write performance. With version 0.64 we explicitly
set the replication level to 1, which is the preferred value for a HPC
environment like ours running on high-end and reliable storage backend hardware
(e.g. DDN SFA10K).

Even with these two changes in place, less-than-ideal write performance and
very poor read performances were observed during our tests.  We also observed
that by increasing the number of IOR processes per client node, the read
performance degraded even further indicating some kind of contention either on
the clients or on the OSD servers.


\subsection{Disabling Client CRC32}

At this point, we were able to both make more client nodes available for Ceph
file system-level testing and also install a profiling tool called \verb!perf!
that is extremely useful for profiling both kernel and user space codes.
Profiling with \verb!perf! showed high CPU utilization on test clients due to
crc32c processing in the Ceph kernel client.  crc32 checksums can be disabled
by changing the CephFS mount options:

\begin{Verbatim}
mount -t ceph 10.37.248.43:6789:/ /mnt/ceph -o name=admin,nocrc
\end{Verbatim}


With client CRC32 disabled, we repeated the IOR tests. New results are shown in
in Figure~\ref{fig:ior-no-client-crc32}. 

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{ior-client-no-crc32}
\caption{IOR test with disabling client-side CRC32}
\label{fig:ior-no-client-crc32}
\end{figure}

We observed that IOR write throughput increased dramatically and is now very
close and comparable to the RADOS Bench performance. Read performance continued
to be poor and continued to scale inversely with the increasing number of
client processes.  Please note that, since these tests were performed, Inktank
has implemented SSE4-based CRC32 code for Intel CPUs.  While any kernel based
CRC32 processing should have already been using SSE4 instructions on Intel
CPUs, this update will allow any user-land Ceph processes to process CRC32
checksums with significantly less overhead.

\subsection{Improving IOR Read Performance}

Deeper analysis with perf showed that there was heavy lock contention during
parallel compaction in the Linux kernel memory manager.  This behavior was first
observed roughly in the kernel 3.5 time frame which was the kernel
installed on our test systems.\footnote{For more information,
please refer to \url{http://lwn.net/Articles/517082/} and
\url{https://patchwork.kernel.org/patch/1338691/}.}

We upgraded our test systems with kernel version 3.9 and performed RADOS Bench
test.  The results were extremely positive and presented in
Figure~\ref{fig:rados-kernel}.


\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{rados-kernel-35vs39}
\caption{RADOS bench: Linux kernel version 3.5 vs. 3.9}
\label{fig:rados-kernel}
\end{figure}



As can be seen, with the 3.9 kernel, while there was a slight improvement on
write performance, read performance improved dramatically.  In addition to the
kernel change, Sage Weil from InkTank suggested increasing the amount of CephFS
client kernel read-ahead cache size as:

\begin{Verbatim}[samepage=true]
mount -t ceph 10.37.248.43:6789:/ /mnt/ceph -o
   name=admin,nocrc,readdir_max_bytes=4104304,readdir_max_entries=8192
\end{Verbatim}

%% SCOTT - is figure 15 IOR over Ceph and 14 is RADOS bench? If so, can we
%% mention that 15 shows the filesystem performance?

IOR results reflecting the read-ahead cache size change are presented in
Figure~\ref{fig:ior-kernel-39}.

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{ior-kernel-39}
\caption{CephFS performance with kernel changes to 3.9, IOR with 4 MB transfer
size}
\label{fig:ior-kernel-39}
\end{figure}


By installing a newer kernel, increasing read-ahead cache size, and increasing the number of
client IOR processes, we were able to achieve very satisfactory I/O performance.


\subsection{Repeating the IOR Scaling Test}

As before, we ran IOR scaling tests with two cases: transfer size 4 KB and 4 MB.
These results are illustrated in Figure~\ref{fig:ior-064}. As expected, we saw
saw  improved read and write performance. These new read and write performance are in
line with observed RADOS bench performance.

%% SCOTT - Can we use the same Y axis for these figures? Any comment from Inktank
%% as to why 4 KB performance is better than 4 MB performance?

\begin{figure}[htb]
\centering
\includegraphics[width=3.5in]{ior-064-4k}
\includegraphics[width=3.5in]{ior-064-4m}
\caption{IOR Scaling Test: 4 KB and 4 MB transfer size}
\label{fig:ior-064}
\end{figure}

Throughout our IOR testing, we observed that the average write throughput is 
lower than the maximum.  This behavior was observed during other tests as well,
indicating that we may have periods of time where write throughput is
temporarily degrading.  Despite these issues, performance generally seems to be
improving with respect to increasing number of clients.  Writes seem to be topping out at
around 5.2 GB/s (which is roughly what we would expect).  Reads seem to be
topping out anywhere from 5.6-7 GB/s, however it is unclear if read performance
would continue scaling with more clients and get closer to ~8 GB/s  we
obervered with RADOS Bench.

