\section{Introduction}

Oak Ridge Leadership Computing Facility (OLCF)\footnote{This research was
supported by, and used the resources of, the Oak Ridge Leadership Computing
Facility, located in the National Center for Computational Sciences at ORNL,
which is managed by UT Battelle, LLC for the U.S. DOE (under the contract No.
DE-AC05-00OR22725).} at Oak Ridge National Laboratory (ORNL) has a long history
of deploying and running very-large-scale high-performance computing (HPC)
systems\footnote{\url{http://www.top500.org/site/48553}}. The facility hosted
the Jaguar supercomputer up until recently, which was upgraded to the Titan
supercomputer. In order to satisfy the I/O demand of such supercomputers (as
well as, other OLCF computational, analysis, and visualization clusters), OLCF
also hosts large-scale file and storage systems. Lustre has been OLCF's choice
as the distributed parallel file system.  The latest incarnation of such
large-scale file systems hosted at OLCF is Spider II\cite{spider2}. Spider II
is the forklift-upgrade of its predecessor, Spider, which was deployed in 2008.
Spider II, deployed in 20013, is designed to operate at over 1 TB/s peak
aggregate I/O throughput and has a 40 PB of raw disk capacity. As the scratch
file system connected to a leadership computing platform at the scale of Titan,
Spider II emphasizes \textit{capability} over \textit{capacity}. However, we
also recognize that there are a wide variety of scientific data and workloads
with different performance and data access requirements (e.g., RESTful
interface, S3-like API, cloud solution integration) which might not be
efficiently serviced by Lustre. OLCF is constantly evaluating new and emerging
file and storage system technologies.  

Ceph\cite{Weil:2006:Ceph} is a distributed storage system designed for
scalability, reliability, and performance.  The system is based on a
distributed object storage service called RADOS (reliable autonomic distributed
object store) that manages the distribution, replication, and migration of
objects.  On top of that reliable storage abstraction Ceph builds a range of
services, including a block storage abstraction (RBD, or rados block device)
and a cache-coherent distributed file system (CephFS).

Data objects are distributed across storage daemons (ceph-osd) using
CRUSH\cite{crush}, a deterministic hashing function that allows administrators
to define flexible placement policies over a hierarchical cluster structure
(e.g., disks, hosts, racks, rows, datacenters).  Because the location of
objects can be calculated based on the object identifier and cluster layout
(similar to consistent hashing~\cite{..}), there is no need for a metadata
index or server for the RADOS object store. Further, because CRUSH provides an
authoritative view of data placement, storage daemons can coordinate directly
to handle data replications, recovery, or object migratation in the face of
failure or cluster topology changes.  A small cluster of monitors (ceph-mon
daemons) use Paxos to provide consensus on the current cluster layout, but do
not need to explicitly coordinate migration or recovery activities.

CephFS builds a distributed cache-coherent file system on top of the object
storage service provided by RADOS.  Files are striped over objects stored and
replicated by RADOS, while a separate cluster of metadata servers (ceph-mds
daemons) manage the file system namespace and coordinate client access to
files.  

Ceph metadata servers store all metadata in RADOS object, which provides a
shared, highly-available, and reliable storage backend.  Unlike many other
distributed file system architectures, Ceph also embeds inodes inside
directories in the common case, allowing entire directories to read from RADOS
into the metadata server cache or prefetched into the client cache using a
single request.

Client hosts that mount the file system communicate with one or more metadata
servers to traverse the namespace but perform file IO by reading and writing
directly to RADOS objects that contain the file data.  The metadata server
cluster monitors the temperature of cached metadata and periodically adjusts
the distribution of the namespace across the MDS cluster by migrating
responsibility for arbitrary subtrees of the hierarchy between a dynamic pool
of active ceph-mds daemons.  This dynamic subtree partitioning~\cite{..}
strategy is both adaptive and highly scalable, allowing additional metadata
server daemons to be added or removed at any time, making it ideally suited
both for large-scale workloads with bursty workloads or general purpose
clusters whose workloads grow or contract over time.

In comparison to other parallel file
systems, Ceph has a number of distinctive features:

\begin{comment}
\begin{itemize}
 
\item Ceph has an intelligent and powerful data placement mechanism, known as
  CRUSH. The CRUSH algorithm allows a client to pre-calculate object
  placement and layout while taking into consideration of failure domains and
  hierarchical storage tiers.
  
  \item From the start, Ceph's design anticipated managing meta data and the
  name space with a cluster of meta data servers. It utilized a dynamic subtree
  partitioning strategy to continuously adapt meta data distribution to current
  demands.

  \item Ceph's design assumes that the system is composed of unreliable
  components; fault-detection and fault-tolerance (e.g., replication) are the
  norm rather than the exception. This is in line with the expectations and
  future directions of Exascale computing.

  \item Ceph is built on top of a unified object management layer, RADOS. Both
  meta data and the file data can take advantage of this uniformity. On top of
  RADOS, Ceph build and project a host of other features such as RESTful
  interface, S3 and Swift-compliant API, cloud integration.

  \item Most of the Ceph processes reside in user-space. Generally speaking,
this makes the system easier to debug and maintain. The client-side support has
long been integrated into Linux mainline kernel, which eases the deployment and
out-of-box experience.

\end{itemize}
\end{comment}

As interesting and feature-rich as Ceph may be,  scalability and performance
are our top priorities due to the unique requirements our environment.  As part
of this study, we set up a dedicated testbed within OLCF to understand and
evaluate Ceph.  Our goal was to investigate the feasibility of using the Ceph
for our future HPC storage deployments. This paper presents our experience,
results, and observations.  While evaluating our results, please keep in mind
that Ceph is still a relatively \textit{young} parallel file system and its
code base is changing rapidly. In between releases, we often experienced
different stability and performance outcomes.  We will try to make clear in the
writing when such changes occurred.

This paper is organized as follows. Section~\ref{sec:testbed} gives an
overview on our general test and evaluation methodology as well as testbed
environment; Following it, Section~\ref{sec:baseline} establishes the baseline
performance and expectations for all critical components on the data path;
Section~\ref{sec:ceph-initial} discusses our early runs, results, and issues
bottom up: from middle tier RADOS object layer to the file system-level and
then meta data performance. In Section~\ref{sec:ceph-tuning} we highlight the
investigative and tuning effort, we compare results before and after, and how
the process eventually bring the system performance to a respectable state.
Finally, Section~\ref{sec:conclusion} summarizes our findings, observations,
and future works.
