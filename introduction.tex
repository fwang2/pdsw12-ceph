\section{Introduction}

Oak Ridge Leadership Computing Facility (OLCF)\footnote{This research was
supported by, and used the resources of, the Oak Ridge Leadership Computing
Facility, located in the National Center for Computational Sciences at ORNL,
which is managed by UT Battelle, LLC for the U.S. DOE (under the contract No.
DE-AC05-00OR22725).} at Oak Ridge National Laboratory (ORNL) has a long history
of deploying and running very-large-scale high-performance computing (HPC)
systems. 
%The facility hosted
%the Jaguar supercomputer up until recently, which was upgraded to the Titan
%supercomputer. 
In order to satisfy the I/O demand of such supercomputers, OLCF
also hosts large-scale file and storage systems. Lustre has been OLCF's choice
as the distributed parallel file system.  
%The latest such
%large-scale file systems is Spider II\cite{spider2}.  Spider II, deployed in
%2013, is designed to operate at over 1 TB/s peak aggregate I/O throughput and
%has a 40 PB of raw disk capacity. As the scratch 
%file system for Titan, a leadership computing platform,
%Spider II emphasizes \textit{capability} over \textit{capacity}. 
However, we
recognize that there are a wide variety of scientific data and emerging
workloads with different performance and data access requirements (e.g., RESTful
interface, S3-like API, cloud solution integration) which might not be
efficiently serviced by Lustre. OLCF is constantly evaluating new and emerging
file and storage system technologies.  

This paper presents our findings on block I/O and file system performance of
Ceph for scientific high-performance computing (HPC) environments. Through
systematic experiments and tuning efforts, we observed that Ceph can perform
close to 80\% of raw hardware bandwidth at object level and close to 70\% of
at file system level. We also identified that Ceph's metadata 
plus data journaling design is a weak spot for a typical

This rest of the paper is organized as follows. Section~\ref{sec:testbed} gives an
overview on our general test and evaluation methodology as well as testbed
environment; Following it, Section~\ref{sec:baseline} establishes the baseline
performance and expectations for all critical components on the data path;
Section~\ref{sec:ceph-initial} discusses our early runs, results, and issues
bottom up: from middle tier RADOS object layer to the file system-level and
then meta data performance. In Section~\ref{sec:ceph-tuning} we highlight the
investigative and tuning effort, we compare results before and after, and how
the process eventually bring the system performance to a respectable state.
Finally, Section~\ref{sec:conclusion} summarizes our findings, observations,
and future works.
